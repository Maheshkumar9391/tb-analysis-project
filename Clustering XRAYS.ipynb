{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNWCf0YbSiWiBMrwvlKemNX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# 1. Install Kaggle library\n","!pip install kaggle\n","\n","# 2. Upload your kaggle.json file\n","from google.colab import files\n","print(\"Please upload your kaggle.json file\")\n","files.upload()\n","\n","# 3. Set up the Kaggle directory and permissions\n","!mkdir -p ~/.kaggle\n","!mv kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json\n","\n","print(\"Kaggle API setup complete.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"3m4olvZn4InT","executionInfo":{"status":"ok","timestamp":1761622916782,"user_tz":300,"elapsed":512822,"user":{"displayName":"Mahesh Kumar Paka","userId":"02734932019503892701"}},"outputId":"89cd539f-561f-4e11-b80a-b8174837709f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\n","Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.10.5)\n","Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n","Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n","Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n","Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n","Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n","Please upload your kaggle.json file\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-d53d6c32-9b01-4c86-ba23-1f77462c38c3\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-d53d6c32-9b01-4c86-ba23-1f77462c38c3\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle (1).json to kaggle (1).json\n","mv: cannot stat 'kaggle.json': No such file or directory\n","chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n","Kaggle API setup complete.\n"]}]},{"cell_type":"code","source":["# Create the directory the Kaggle API expects\n","!mkdir -p ~/.kaggle\n","\n","# Copy your key file to the correct location with the correct name\n","!cp 'kaggle (1).json' ~/.kaggle/kaggle.json\n","\n","# Set the required permissions so the API will read the file\n","!chmod 600 ~/.kaggle/kaggle.json"],"metadata":{"id":"lqYTtXm46JOm","executionInfo":{"status":"ok","timestamp":1761623677933,"user_tz":300,"elapsed":314,"user":{"displayName":"Mahesh Kumar Paka","userId":"02734932019503892701"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Download the dataset from Kaggle\n","\n","!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n","\n","# Unzip the downloaded file\n","print(\"Unzipping dataset...\")\n","!unzip -q chest-xray-pneumonia.zip\n","\n","# List the contents to confirm\n","!ls\n","\n","print(\"Dataset is ready in the 'chest_xray' folder.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yFrlh8_m9HPM","executionInfo":{"status":"ok","timestamp":1761623768285,"user_tz":300,"elapsed":63129,"user":{"displayName":"Mahesh Kumar Paka","userId":"02734932019503892701"}},"outputId":"43858a60-7b6a-47ae-8578-e57dabd78cad"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset URL: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n","License(s): other\n","Downloading chest-xray-pneumonia.zip to /content\n"," 99% 2.28G/2.29G [00:19<00:00, 255MB/s]\n","100% 2.29G/2.29G [00:19<00:00, 125MB/s]\n","Unzipping dataset...\n"," chest_xray   chest-xray-pneumonia.zip\t'kaggle (1).json'   sample_data\n","Dataset is ready in the 'chest_xray' folder.\n"]}]},{"cell_type":"code","source":["# --- Cell 1: Imports and Helper Functions ---\n","import cv2\n","import numpy as np\n","import os\n","from skimage.feature import hog, local_binary_pattern\n","from sklearn.preprocessing import StandardScaler\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n","from sklearn.metrics import silhouette_score, davies_bouldin_score\n","from scipy.cluster.hierarchy import dendrogram, linkage\n","from sklearn.neighbors import NearestNeighbors\n","from sklearn.manifold import TSNE\n","from sklearn.decomposition import PCA\n","import pandas as pd\n","import seaborn as sns\n","import warnings\n","\n","# Suppress warnings for cleaner output\n","warnings.filterwarnings('ignore')\n","\n","# --- Part 1: Data Acquisition and Preprocessing ---\n","\n","def preprocess_image(image_path, size=(256, 256)):\n","    \"\"\"Loads, converts to grayscale, and resizes an image.\"\"\"\n","    try:\n","        img = cv2.imread(image_path)\n","        if img is None:\n","\n","            from google.colab.patches import cv2_imshow\n","            print(f\"Warning: Could not read image {image_path}\")\n","            return None\n","\n","        if len(img.shape) == 3 and img.shape[2] == 3:\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","\n","        img_resized = cv2.resize(img, size, interpolation=cv2.INTER_AREA)\n","        return img_resized\n","    except Exception as e:\n","        print(f\"Error processing {image_path}: {e}\")\n","        return None\n","\n","def load_data(base_dir):\n","    \"\"\"Loads all images from train, test, and val sets and assigns labels.\"\"\"\n","    images = []\n","    labels = []\n","\n","    sets = ['train', 'test', 'val']\n","    categories = ['NORMAL', 'PNEUMONIA']\n","\n","    for s in sets:\n","        for i, category in enumerate(categories):\n","            path = os.path.join(base_dir, s, category)\n","            if not os.path.isdir(path):\n","                print(f\"Warning: Directory not found {path}\")\n","                continue\n","\n","            for filename in os.listdir(path):\n","                if filename.endswith('.jpeg') or filename.endswith('.png'):\n","                    img_path = os.path.join(path, filename)\n","                    img = preprocess_image(img_path)\n","                    if img is not None:\n","                        images.append(img)\n","                        labels.append(i) # 0 for NORMAL, 1 for PNEUMONIA\n","\n","    return images, np.array(labels)\n","\n","# --- Part 2: Feature Vector Generation ---\n","\n","def extract_features(image):\n","    \"\"\"Extracts a concatenated feature vector (Hist, HOG, LBP, Hu) from a single image.\"\"\"\n","\n","    # 1. Histogram (v_hist)\n","    v_hist = cv2.calcHist([image], [0], None, [256], [0, 256]).flatten()\n","\n","    # 2. HOG (v_hog)\n","    v_hog = hog(image, orientations=9, pixels_per_cell=(8, 8),\n","                cells_per_block=(2, 2), visualize=False).flatten()\n","\n","    # 3. LBP (v_lbp)\n","    P = 8\n","    R = 1\n","    lbp = local_binary_pattern(image, P, R, method='uniform')\n","    (v_lbp, _) = np.histogram(lbp.ravel(),\n","                              bins=np.arange(0, P + 3),\n","                              range=(0, P + 2))\n","    v_lbp = v_lbp.astype(\"float\")\n","\n","    # 4. Hu Moments (v_hu)\n","    moments = cv2.moments(image)\n","    v_hu = cv2.HuMoments(moments).flatten()\n","\n","    combined_features = np.hstack([v_hist, v_hog, v_lbp, v_hu])\n","    return combined_features"],"metadata":{"id":"FPMmP2Yg6UtB","executionInfo":{"status":"ok","timestamp":1761623805083,"user_tz":300,"elapsed":3,"user":{"displayName":"Mahesh Kumar Paka","userId":"02734932019503892701"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# --- Cell 2: Main Execution ---\n","\n","# 1. Load Data\n","# This is the correct path in Colab after unzipping\n","DATASET_PATH = 'chest_xray'\n","print(\"Loading and preprocessing images...\")\n","images, y_true = load_data(DATASET_PATH)\n","print(f\"Loaded {len(images)} images.\")\n","\n","# 2. Extract Features (This will take a few minutes)\n","print(\"Extracting features from all images...\")\n","all_features = []\n","for img in tqdm(images):\n","    features = extract_features(img)\n","    all_features.append(features)\n","\n","X = np.array(all_features)\n","print(f\"Feature matrix shape: {X.shape}\")\n","\n","# 3. Normalization\n","print(\"Normalizing feature matrix...\")\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","print(\"Setup complete. X_scaled and y_true are ready.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0OyD-5yS6h8I","executionInfo":{"status":"ok","timestamp":1761624169914,"user_tz":300,"elapsed":358851,"user":{"displayName":"Mahesh Kumar Paka","userId":"02734932019503892701"}},"outputId":"53bb800e-0ed0-4586-e5c9-0bc182d84922"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading and preprocessing images...\n","Loaded 5856 images.\n","Extracting features from all images...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5856/5856 [04:34<00:00, 21.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Feature matrix shape: (5856, 34869)\n","Normalizing feature matrix...\n","Setup complete. X_scaled and y_true are ready.\n"]}]},{"cell_type":"code","source":["# --- Cell 3: Clustering (Part 3) ---\n","\n","K = 2 # Target K\n","results = {}\n","metrics = {}\n","\n","# 1. K-Means\n","print(\"\\nRunning K-Means...\")\n","inertias = []\n","K_range = range(2, 11)\n","for k_i in K_range:\n","    km = KMeans(n_clusters=k_i, random_state=42, n_init=10).fit(X_scaled)\n","    inertias.append(km.inertia_)\n","\n","plt.figure(figsize=(8, 4))\n","plt.plot(K_range, inertias, 'bx-')\n","plt.xlabel('Number of clusters (K)')\n","plt.ylabel('Inertia')\n","plt.title('K-Means Elbow Method')\n","plt.show()\n","\n","kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)\n","kmeans_labels = kmeans.fit_predict(X_scaled)\n","results['K-Means'] = kmeans_labels\n","print(f\"K-Means (K={K}) complete.\")\n","\n","\n","# 2. Hierarchical Clustering (Agglomerative)\n","print(\"\\nRunning Hierarchical Clustering...\")\n","# Use a sample for the dendrogram, as 5800+ points is too much to plot\n","sample_size = min(5000, X_scaled.shape[0])\n","# Fix for potential issue if dataset is smaller than 5000\n","np.random.seed(42) # for reproducibility\n","sample_indices = np.random.choice(X_scaled.shape[0], sample_size, replace=False)\n","X_sample = X_scaled[sample_indices]\n","\n","print(\"Calculating linkage for dendrogram...\")\n","linkage_matrix = linkage(X_sample, method='ward')\n","\n","plt.figure(figsize=(15, 7))\n","plt.title('Hierarchical Clustering Dendrogram (Sampled Data)')\n","plt.xlabel('Sample index')\n","plt.ylabel('Distance (Ward)')\n","dendrogram(linkage_matrix, p=5, truncate_mode='lastp', show_leaf_counts=True)\n","plt.show()\n","\n","agglo = AgglomerativeClustering(n_clusters=K, linkage='ward')\n","agglo_labels = agglo.fit_predict(X_scaled)\n","results['Hierarchical (Ward)'] = agglo_labels\n","print(\"Hierarchical Clustering complete.\")\n","\n","\n","# 3. DBSCAN\n","print(\"\\nRunning DBSCAN...\")\n","min_pts = 10\n","nn = NearestNeighbors(n_neighbors=min_pts)\n","nn.fit(X_scaled)\n","distances, indices = nn.kneighbors(X_scaled)\n","\n","k_distances = np.sort(distances[:, min_pts-1], axis=0)\n","\n","plt.figure(figsize=(8, 4))\n","plt.plot(k_distances)\n","plt.ylabel(f\"{min_pts}-th Nearest Neighbor Distance\")\n","plt.xlabel(\"Points sorted by distance\")\n","plt.title(\"DBSCAN k-distance graph (for finding eps)\")\n","plt.grid(True)\n","plt.show()\n","\n","# The y-value at that elbow is your EPSILON.\n","\n","EPSILON = 30.0 #TUNE THIS VALUE BASED ON YOUR K-DISTANCE PLOT\n","# -------------------------\n","\n","dbscan = DBSCAN(eps=EPSILON, min_samples=min_pts)\n","dbscan_labels = dbscan.fit_predict(X_scaled)\n","results['DBSCAN'] = dbscan_labels\n","n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n","n_noise = list(dbscan_labels).count(-1)\n","print(f\"DBSCAN complete. Found {n_clusters_dbscan} clusters and {n_noise} noise points.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"smCDlzLn_Bic","outputId":"0943c597-1bb1-4ffe-fa98-0b2033ee9111"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Running K-Means...\n"]}]},{"cell_type":"code","source":["# --- Cell 4: Evaluation (Part 4) ---\n","\n","# 1. Dimensionality Reduction for Visualization (t-SNE)\n","print(\"\\nRunning t-SNE for visualization (this will take a few minutes)...\")\n","\n","# We can run PCA first to reduce to 50 dims, then t-SNE. This is much faster.\n","pca = PCA(n_components=50, random_state=42)\n","X_pca = pca.fit_transform(X_scaled)\n","\n","tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)\n","X_2D = tsne.fit_transform(X_pca) # Fit t-SNE on the PCA-reduced data\n","print(\"t-SNE complete.\")\n","\n","# 2. Plotting Cluster Visualizations\n","fig, axes = plt.subplots(1, 4, figsize=(28, 7), sharex=True, sharey=True)\n","fig.suptitle('Cluster Visualization using t-SNE (on PCA-reduced data)', fontsize=20)\n","\n","# Plot 1: Ground Truth\n","sns.scatterplot(ax=axes[0], x=X_2D[:, 0], y=X_2D[:, 1],\n","                hue=y_true,\n","                palette=['#1f77b4', '#ff7f0e'], # Blue, Orange\n","                legend='full')\n","axes[0].set_title(f'Ground Truth (0=NORMAL, 1=PNEUMONIA)')\n","\n","# Plot 2: K-Means\n","sns.scatterplot(ax=axes[1], x=X_2D[:, 0], y=X_2D[:, 1],\n","                hue=results['K-Means'],\n","                palette='viridis',\n","                legend='full')\n","axes[1].set_title(f'K-Means (K={K})')\n","\n","# Plot 3: Hierarchical\n","sns.scatterplot(ax=axes[2], x=X_2D[:, 0], y=X_2D[:, 1],\n","                hue=results['Hierarchical (Ward)'],\n","                palette='viridis',\n","                legend='full')\n","axes[2].set_title(f'Hierarchical (K={K})')\n","\n","# Plot 4: DBSCAN\n","sns.scatterplot(ax=axes[3], x=X_2D[:, 0], y=X_2D[:, 1],\n","                hue=results['DBSCAN'],\n","                palette='deep', # 'deep' handles noise (-1) well\n","                legend='full')\n","axes[3].set_title(f'DBSCAN (Clusters={n_clusters_dbscan})')\n","\n","plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","plt.show()\n","\n","# 3. Calculate Intrinsic Metrics\n","print(\"\\nCalculating metrics...\")\n","metric_results = []\n","\n","for name, labels in results.items():\n","    if len(set(labels)) < 2:\n","        print(f\"Skipping metrics for {name}: only 1 cluster found.\")\n","        continue\n","\n","    if name == 'DBSCAN':\n","        mask = labels != -1\n","        if len(set(labels[mask])) < 2:\n","            print(f\"Skipping metrics for {name}: not enough non-noise clusters.\")\n","            continue\n","        X_eval = X_scaled[mask]\n","        labels_eval = labels[mask]\n","    else:\n","        X_eval = X_scaled\n","        labels_eval = labels\n","\n","    sil = silhouette_score(X_eval, labels_eval)\n","    db = davies_bouldin_score(X_eval, labels_eval)\n","\n","    metric_results.append({\n","        \"Method\": name,\n","        \"Silhouette Score\": sil,\n","        \"Davies-Bouldin Index\": db\n","    })\n","\n","# Display metrics table\n","metrics_df = pd.DataFrame(metric_results)\n","print(\"\\n--- Final Metrics Table ---\")\n","print(metrics_df.to_markdown(index=False))"],"metadata":{"id":"2ak0zO3p_ckx"},"execution_count":null,"outputs":[]}]}